First I read a little bit of OpenCV docs until I find how to resize images, I read os.sep and os.path.join on python docs to make the function platform-independent, then I implemented the load_data without any problems using inter_area to resize images, then to take less time when testing the neural network structure, I used pickle library to save the X_train, y_train, X_test and y_test variables to disk and load them when experimenting with AI designs.
Then I was having some problems installing TensorFlow, so I went to read the documentation and found that TensorFlow doesn't support Python 3.9, so I downgraded to Python 3.8.6 and installed TensorFlow, although it wasn't finding my GPU, so I went through the documentation again and installed some prerequisites, and everything went well.
To find a good AI design, I was thinking to do an algorithm that tests all the possible architecture, so I made one CSV file and list some possible options of numbers of convolutional and pooling layers, numbers of filters, kernel sizes, pool sizes, activations, losses, optimizers, numbers of hidden layers, the sizes of these layers and dropout values, but when I saw that those options would have millions of possible combinations, I gave up on that idea because each combination would take around a minute to train, so it would be unfeasible, so I started randomly test some designs, but the AIs has had very high losses and very low accuracy (7% on average), so I decided to read some articles on how to design a CNN and realized that I wasn't using softmax activation in the output layer, so after I fixed it, it started to have higher accuracy.
After testing with two convolutional and pooling layers and four hidden layers with a size of 128 I found that elu activation function, nadam optimizer and four hidden layers with a size of 128 without dropout was a great choice, then I tested with various kernels sizes, numbers of filters and pool sizes and found that three convolutional layers with a size of 128 and kernel sizes of 4x4, 3x3 and 2x2 respectively and three average-pooling layers with pool-sizes of 3x3, 2x2 and 2x2 respectively was a good choice, but I noticed a little bit of overfitting so, I tried some different numbers and sizes of hidden layers and end up with three hidden layers of sizes and dropouts of 128 / 0.25, 256 / 0.5 and 128 / 0.25 respectively.